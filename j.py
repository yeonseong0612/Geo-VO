import torch
import torch.nn as nn
from utils.DBA_utils import compute_projection_jacobian
from src.model import VO
from CFG.vo_cfg import vo_cfg
from lietorch import SE3

def test_jacobian_precision():
    # 1. í™˜ê²½ ì„¤ì •
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    B, N = 2, 800
    eps = 1e-3  # ë¯¸ì„¸ ë³€í™”ëŸ‰ (ë„ˆë¬´ ì‘ìœ¼ë©´ float32 ì •ë°€ë„ ë¬¸ì œ, ë„ˆë¬´ í¬ë©´ ì„ í˜• ê·¼ì‚¬ ë¬¸ì œ)
    
    # 2. ê°€ìƒì˜ ì…ë ¥ ë°ì´í„° (í˜„ì‹¤ì ì¸ ì£¼í–‰ ìƒí™© ê°€ì •)
    # ì´ë¯¸ì§€ ì¤‘ì•™ ê·¼ì²˜ì˜ í‚¤í¬ì¸íŠ¸ë“¤
    kpts = torch.randn(B, N, 2, device=device) * 50 + 400 
    # 10ë¯¸í„° ì•ì˜ í‰ë©´
    depth = torch.ones(B, N, 1, device=device) * 10.0
    # í‘œì¤€ì ì¸ ì¹´ë©”ë¼ íŒŒë¼ë¯¸í„°
    intrinsics = torch.tensor([[600.0, 600.0, 400.0, 300.0]], device=device).repeat(B, 1)
    
    # 3. ë¶„ì„ì  ìì½”ë¹„ì•ˆ ê³„ì‚° (ìš°ë¦¬ê°€ ë§Œë“  í•¨ìˆ˜)
    J_p, _ = compute_projection_jacobian(kpts, depth, intrinsics)
    
    # ê²€ì¦í•  íŒŒë¼ë¯¸í„° ì„ íƒ: Translation Z (tz)
    # J_pì˜ ì¸ë±ìŠ¤: 0:tx, 1:ty, 2:tz, 3:rx, 4:ry, 5:rz
    ana_grad_tz = J_p[..., 2] 

    # 4. ìˆ˜ì¹˜ì  ë¯¸ë¶„ ê³„ì‚° (Central Difference ë°©ì‹)
    # f'(x) â‰ˆ (f(x + eps) - f(x - eps)) / (2 * eps)
    vo_tester = VO(vo_cfg).to(device)
    identity = SE3.Identity(B, device=device)

    # (A) f(x + eps) ê³„ì‚°
    delta_plus = torch.zeros(B, 6, device=device)
    delta_plus[:, 2] = eps # tz ë°©í–¥ìœ¼ë¡œ +eps
    p_plus = vo_tester.projector(kpts, depth, SE3.exp(delta_plus) * identity, intrinsics)

    # (B) f(x - eps) ê³„ì‚°
    delta_minus = torch.zeros(B, 6, device=device)
    delta_minus[:, 2] = -eps # tz ë°©í–¥ìœ¼ë¡œ -eps
    p_minus = vo_tester.projector(kpts, depth, SE3.exp(delta_minus) * identity, intrinsics)

    # (C) ìˆ˜ì¹˜ì  ê¸°ìš¸ê¸° ì‚°ì¶œ
    num_grad_tz = (p_plus - p_minus) / (2 * eps)

    # 5. ê²°ê³¼ ë¹„êµ ë° ì¶œë ¥
    diff = torch.abs(num_grad_tz - ana_grad_tz)
    mean_error = diff.mean().item()
    max_error = diff.max().item()

    print("\n" + "="*60)
    print("ğŸ” ìì½”ë¹„ì•ˆ ì •ë°€ ê²€ì¦ ê²°ê³¼ (Central Difference)")
    print("="*60)
    print(f"Numerical Gradient (tz) Mean:  {num_grad_tz.mean().item():.8f}")
    print(f"Analytical Gradient (tz) Mean: {ana_grad_tz.mean().item():.8f}")
    print("-" * 60)
    print(f"Mean Absolute Error (MAE):     {mean_error:.10f}")
    print(f"Max Absolute Error:             {max_error:.10f}")
    print("="*60)

    if mean_error < 1e-3:
        print("âœ… ê²°ê³¼: ìì½”ë¹„ì•ˆ ìˆ˜ì‹ì´ ìˆ˜ì¹˜ì ìœ¼ë¡œ ì™„ë²½í•˜ê²Œ ì¼ì¹˜í•©ë‹ˆë‹¤!")
    else:
        print("âš ï¸ ì£¼ì˜: ì˜¤ì°¨ê°€ ì—¬ì „í•©ë‹ˆë‹¤. ìˆ˜ì‹ì˜ ë¶€í˜¸ë‚˜ ë‹¨ìœ„ë¥¼ ì¬ì ê²€í•˜ì„¸ìš”.")
    print("="*60 + "\n")

if __name__ == "__main__":
    test_jacobian_precision()