import torch
import torch.nn as nn
import torch.nn.functional as F
from lietorch import SE3
from torch_geometric.nn import GATv2Conv

import torch
import torch.nn as nn

from utils.geo_utils import *

class GeometricGAT(nn.Module):
    def __init__(self, in_channels, hidden_dim=256, heads=4, pos_dim=3):
        super().__init__()
        # 1. ì…ë ¥ë°›ì€ in_channelsê°€ 256ì¸ì§€ 320ì¸ì§€ ëª…í™•íˆ í•´ì•¼ í•©ë‹ˆë‹¤.
        # ë§Œì•½ PoseInitializerì—ì„œ 320ì„ ë„£ì–´ì¤€ë‹¤ë©´, ì—¬ê¸°ì„œ 64ë¥¼ ë”í•˜ë©´ ì•ˆ ë©ë‹ˆë‹¤.
        
        # [ì¶”ì²œ] ì°¨ì›ì„ ëª…ì‹œì ìœ¼ë¡œ ê³„ì‚°
        self.node_base_dim = in_channels # PoseInitializerì—ì„œ 256ì„ ë„£ì–´ì¤€ë‹¤ê³  ê°€ì •
        self.pos_dim = 64
        self.total_in_dim = self.node_base_dim + self.pos_dim # 256 + 64 = 320

        self.pos_encoder = nn.Sequential(
            nn.Linear(pos_dim, 32),
            nn.LayerNorm(32), 
            nn.SiLU(),
            nn.Linear(32, self.pos_dim),
            nn.LayerNorm(self.pos_dim)
        )

        # GATv2Convì˜ ì…ë ¥ì€ ë°˜ë“œì‹œ x_combinedì˜ ì°¨ì›ì¸ 320ì´ì–´ì•¼ í•©ë‹ˆë‹¤.
        self.conv = GATv2Conv(
            self.total_in_dim, # <--- ì—¬ê¸°ê°€ 384ë¡œ ë˜ì–´ìˆì„ ê°€ëŠ¥ì„±ì´ í½ë‹ˆë‹¤. 320ìœ¼ë¡œ ê³ ì •!
            hidden_dim // heads,
            heads=heads, 
            edge_dim=3,
            add_self_loops=False
        )
        self.res_proj = nn.Linear(self.total_in_dim, hidden_dim)
        
        self.norm = nn.LayerNorm(hidden_dim)
        self.post_norm = nn.LayerNorm(hidden_dim)
        self.projector = nn.Linear(hidden_dim, 256)
        self.SiLU = nn.SiLU()

    def forward(self, x, edge_index, kpts, pts_3d, edge_attr=None):
        device = x.device
        
        # [ìˆ˜ì • 1] ì…ë ¥ ë°ì´í„° ìì²´ì— ì•„ì£¼ ë¯¸ì„¸í•œ ë…¸ì´ì¦ˆ ì¶”ê°€ (LayerNorm/Norm ë³´í˜¸)
        # 0ì¸ ë°ì´í„°ê°€ 0ì¸ ìƒíƒœë¡œ ì—°ì‚°ì— ë“¤ì–´ê°€ëŠ” ê²ƒì„ ì›ì²œ ì°¨ë‹¨
        kpts = kpts + torch.randn_like(kpts) * 1e-7
        pts_3d = pts_3d + torch.randn_like(pts_3d) * 1e-7
        
        # [ìˆ˜ì • 2] ì¢Œí‘œ ì •ê·œí™” ë° ê¹Šì´ ì œí•œ
        norm_uv = kpts / torch.tensor([1216.0, 352.0], device=device)
        depth = torch.clamp(pts_3d[:, 2:3], min=0.1, max=100.0)
        
        # [Step 1] ìœ„ì¹˜ íŠ¹ì§• ì¶”ì¶œ (LayerNorm í­ì£¼ ë°©ì§€ìš© epsilon)
        pos_input = torch.cat([norm_uv, depth], dim=-1)
        pos_feat = self.pos_encoder(pos_input)
        
        # [N, 320] ê²°í•©
        x_combined = torch.cat([x, pos_feat], dim=-1)

        # [Step 2] Edge_info ê³„ì‚° (Gradient NaN ë³´í˜¸)
        if edge_attr is None and edge_index is not None:
            src, dst = edge_index[0], edge_index[1]
            rel_uv = norm_uv[dst] - norm_uv[src]
            # ì•ˆì „í•œ norm ê³„ì‚°
            dist = torch.sqrt(torch.sum(rel_uv**2, dim=-1, keepdim=True) + 1e-9)
            edge_attr = torch.cat([rel_uv, dist], dim=-1)

        # [Step 4] GAT ì—°ì‚° - NaN ì „íŒŒ ë°©ì§€
        if torch.isnan(x_combined).any():
            x_combined = torch.where(torch.isnan(x_combined), torch.zeros_like(x_combined), x_combined)

        out, _ = self.conv(x_combined, edge_index, edge_attr, return_attention_weights=True)
        
        # [Step 5] Residual Connection & Final Proj
        out = self.norm(out)
        identity = self.res_proj(x_combined)
        
        # í•©ì‚° ì§ì „ ì²´í¬
        out = self.post_norm(self.SiLU(out + identity)) 
        out = self.projector(out)
        
        return out, None, edge_attr

class TriangleHead(nn.Module):
    def __init__(self, node_dim=256, hidden_dim=128):
        super().__init__()
        self.mlp = nn.Sequential(
            nn.Linear(node_dim * 3, hidden_dim),
            nn.SiLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.SiLU()
        )
        
        # Weight(0~1) : Confidence of Triangle
        self.weight_head = nn.Sequential(
            nn.Linear(hidden_dim, 1),
            nn.Sigmoid()
        )
        
        # Normal Vector
        self.normal_head = nn.Linear(hidden_dim, 3) 

    def forward(self, node_feat, tri_indices):
        # 1. node_feat ì°¨ì› ê°•ì œ ë³´ì • [B, N, C]
        if node_feat.dim() == 2: # [N, C]ì¸ ê²½ìš° ë°°ì¹˜ ì°¨ì› ì¶”ê°€
            node_feat = node_feat.unsqueeze(0)
            
        B = node_feat.shape[0]
        all_weights = []
        all_normals = []

        for b in range(B):
            # [ìˆ˜ì •] tri_indices[b]ê°€ í…ì„œì¸ì§€ ë¦¬ìŠ¤íŠ¸ì¸ì§€ì— ë”°ë¼ ì•ˆì „í•˜ê²Œ ì²˜ë¦¬
            tris_full = tri_indices[b] # [max_T, 3]
            mask = tris_full[:, 0] > -1 # -1ì´ ì•„ë‹Œ ìœ íš¨í•œ í–‰ë§Œ ì„ íƒ
            tris = tris_full[mask]
            if isinstance(tris, list):
                tris = torch.tensor(tris, device=node_feat.device)
            
            # ì‚¼ê°í˜•ì´ ì—†ëŠ” ê²½ìš° ì˜ˆì™¸ ì²˜ë¦¬
            if tris.shape[0] == 0:
                all_weights.append(torch.zeros((0, 1), device=node_feat.device))
                all_normals.append(torch.zeros((0, 3), device=node_feat.device))
                continue

            # [í•µì‹¬] ë°°ì¹˜ ì¸ë±ì‹±ì„ ëª…í™•í•˜ê²Œ ìˆ˜í–‰
            # node_feat[b] -> [N, 256]
            f1 = node_feat[b, tris[:, 0]] # [T, 256]
            f2 = node_feat[b, tris[:, 1]] # [T, 256]
            f3 = node_feat[b, tris[:, 2]] # [T, 256]

            # 2. Concat Feature [T, 768]
            f_tri = torch.cat([f1, f2, f3], dim=-1)

            # 3. MLP ìˆ˜í–‰
            feat = self.mlp(f_tri)

            # 4. Result ê³„ì‚°
            weights = self.weight_head(feat)    # [T, 1]
            normals = self.normal_head(feat)    # [T, 3]
            # [ì¶”ê°€] ì •ê·œí™” ì‹œ 0ìœ¼ë¡œ ë‚˜ëˆ„ê¸° ë°©ì§€
            norm_val = torch.norm(normals, p=2, dim=-1, keepdim=True)
            normals = torch.where(norm_val > 1e-8, normals / norm_val, torch.zeros_like(normals))

            all_weights.append(weights)
            all_normals.append(normals)
            
        return all_weights, all_normals
        
class PoseInitializer(nn.Module):
    def __init__(self, in_channels=256, node_dim=256):
        super().__init__()
        self.gat = GeometricGAT(in_channels=in_channels, hidden_dim=256)
        self.tri_head = TriangleHead(node_dim=node_dim)

    def forward(self, descs, kpts, pts_3d, tri_indices, kpts_tp1, intrinsics):
        B, N, _ = descs.shape
        device = descs.device

        # [ìˆ˜ì • 1] ë°°ì¹˜ ì „ì²´ì˜ ìœ íš¨í•œ ì‚¼ê°í˜•ë§Œ ëª¨ì•„ì„œ ì—ì§€ë¥¼ ë§Œë“­ë‹ˆë‹¤.
        # tri_indicesê°€ [B, max_T, 3] í…ì„œì´ë¯€ë¡œ, ë§ˆìŠ¤í¬ë¥¼ í†µí•´ ë¦¬ìŠ¤íŠ¸ë¡œ ë¶„ë¦¬í•©ë‹ˆë‹¤.
        valid_tris_list = []
        for b in range(B):
            tris_full = tri_indices[b]
            mask = tris_full[:, 0] > -1
            valid_tris_list.append(tris_full[mask])

        # [Step 1] GAT : ì—ì§€ ìƒì„± ë° íŠ¹ì§• ì—…ë°ì´íŠ¸
        # tri_indices_to_edges ë‚´ë¶€ì—ì„œ ë°°ì¹˜ ì˜¤í”„ì…‹ ì²˜ë¦¬ê°€ ë˜ì–´ìˆì–´ì•¼ í•©ë‹ˆë‹¤.
        edges = tri_indices_to_edges(valid_tris_list, B, N, device)
        
        node_feat_flat, _, edge_attr = self.gat(
            x=descs.view(-1, 256), edge_index=edges,
            kpts=kpts.view(-1, 2), pts_3d=pts_3d.view(-1, 3)
        )
        node_feat = node_feat_flat.view(B, N, 256)
        
        # [Step 2] Triangle Weights (ìœ íš¨í•œ ì‚¼ê°í˜• ë¦¬ìŠ¤íŠ¸ ì „ë‹¬)
        weights_list, _ = self.tri_head(node_feat, valid_tris_list)

        final_R_list, final_tri_weights, final_vp_conf = [], [], []

        # [Step 3] ë°°ì¹˜ë³„ ì´ˆê¸° Pose ê²°ì • ë£¨í”„
        for b in range(B):
            tris = valid_tris_list[b].to(device) # ì´ë¯¸ ë§ˆìŠ¤í‚¹ëœ ì‚¼ê°í˜• ì‚¬ìš©
            raw_w = weights_list[b]
            
            # [Step 2] ì •ê·œí™” ì¢Œí‘œ ê³„ì‚° (ë¯¸ë¶„ ë¶ˆí•„ìš”)
            fx, fy, cx, cy = intrinsics[b]
            p_norm = torch.stack([
                (kpts_tp1[b, :, 0] - cx) / (fx + 1e-8),
                (kpts_tp1[b, :, 1] - cy) / (fy + 1e-8),
                torch.ones(N, device=device)
            ], dim=-1)

            # [Step 3] ì†Œì‹¤ì  íˆ¬í‘œ (ìˆ˜ì¹˜ ì•ˆì •ì„±ì„ ìœ„í•´ ë¯¸ë¶„ ëŠê¸°)
            # w_static: íˆ¬í‘œ ë¡œì§ì´ GATë¥¼ ì§ì ‘ í”ë“œëŠ” ê²ƒì„ ë°©ì§€
            w_static = raw_w.detach() 
            K_j = compute_individual_Kj(tris, pts_3d[b], p_norm) + torch.randn(tris.shape[0], 3, 3, device=device)*1e-5
            if torch.isnan(K_j).any(): print(f"!!! Batch {b}: K_j has NaN")
            R_cands = batch_svd(K_j)
            if torch.isnan(R_cands).any(): 
                print(f"ğŸš¨ Batch {b}: batch_svd output has NaN! Check compute_individual_Kj.")
            
            # xv_j: ë‚˜ëˆ—ì…ˆ í­ì£¼ ë°©ì§€
            xv_j = fx * (R_cands[:, 0, 2] / torch.clamp(R_cands[:, 2, 2], min=0.01)) + cx
            xv_j = torch.clamp(xv_j, -2000, 4000)
            
            # xv_star: ìµœì  ì†Œì‹¤ì  (ìƒìˆ˜ë¡œ ì·¨ê¸‰)
            xv_star = differentiable_voting(xv_j, w_static, sigma=2.0).detach() 

            # s_j: íˆ¬í‘œ ì‹ ë¢°ë„ (ë¯¸ë¶„ ì°¨ë‹¨)
            dist_sq = torch.clamp((xv_j - xv_star)**2, max=100.0)
            s_j = torch.exp(-dist_sq / (2 * 2.0**2)).unsqueeze(-1).detach()

            # [Step 4] ìµœì¢… R_init ê³„ì‚° (GATë¡œ ë¯¸ë¶„ì´ íë¥´ëŠ” í•µì‹¬ êµ¬ê°„)
            # s_jëŠ” ìƒìˆ˜ë¡œ ì·¨ê¸‰í•˜ì—¬ GATê°€ 'ì–´ë–¤ ì‚¼ê°í˜•ì´ íˆ¬í‘œë¥¼ ì˜í–ˆëŠ”ì§€'ì— ì§‘ì¤‘í•˜ê²Œ í•¨
            combined_w = raw_w * s_j 
            R_init = estimate_rotation_svd_differentiable(combined_w, tris, pts_3d[b], p_norm)
            if torch.isnan(R_init).any():
                print(f"ğŸš¨ Batch {b}: R_init is NaN! SVD gradient might be exploding.")

            # [Step 5] ê²°ê³¼ ì •ë¦¬
            v_conf = torch.tanh(torch.zeros((N, 1), device=device).scatter_add_(
                0, tris.view(-1, 1).expand(-1, 1), s_j.repeat_interleave(3, dim=0)
            ))

            final_R_list.append(R_init)
            final_tri_weights.append(combined_w)
            final_vp_conf.append(v_conf)

        return torch.stack(final_R_list), final_tri_weights, torch.stack(final_vp_conf), edges, edge_attr
    
class DBASolver(nn.Module):
    def __init__(self): 
        super().__init__()

    def forward(self, r, w, J_p, J_d, lmbda, iter_idx):
        """
        iter_idx: í˜„ì¬ ìµœì í™” ë£¨í”„ì˜ ì¸ë±ìŠ¤ (0ë¶€í„° ì‹œì‘)
        """
        B, N, _ = r.shape
        device = r.device
        
        # 1. lmbda ë°©ì–´
        safe_lmbda = torch.where(torch.isnan(lmbda), torch.tensor(1e2, device=device), lmbda)
        safe_lmbda = torch.clamp(safe_lmbda, min=1e-3)

        # 2. ê°€ì¤‘ì¹˜ ì²˜ë¦¬
        conf = w[..., 0:1].unsqueeze(-1)    
        node_lambda = w[..., 1:2] if w.shape[-1] >= 2 else torch.zeros((B, N, 1), device=device)

        # 3. Hessian ë° Gradient ê³„ì‚°
        H_pp = torch.matmul(J_p.transpose(-1, -2), conf * J_p).sum(dim=1)
        H_pd = torch.matmul(J_p.transpose(-1, -2), conf * J_d)
        H_dd = torch.matmul(J_d.transpose(-1, -2), conf * J_d).squeeze(-1)

        g_p = torch.matmul(J_p.transpose(-1, -2), conf * r.unsqueeze(-1)).sum(dim=1) 
        g_d = torch.matmul(J_d.transpose(-1, -2), conf * r.unsqueeze(-1)).squeeze(-1) 

        # 4. Levenberg-Marquardt Damping
        diag_mask = torch.eye(6, device=device).unsqueeze(0)
        H_pp = H_pp + (safe_lmbda * diag_mask) 
        
        H_dd_safe = torch.clamp(H_dd + safe_lmbda + node_lambda + 1e-4, min=1e-4)
        inv_H_dd = 1.0 / H_dd_safe
        
        H_pd_invHdd = H_pd * inv_H_dd.view(B, N, 1, 1)
        term_to_sub = torch.matmul(H_pd_invHdd, H_pd.transpose(-1, -2)).sum(dim=1)
    
        # 5. Reduced Camera System (Schur Complement)
        H_eff = H_pp - term_to_sub
        g_eff = g_p - (H_pd_invHdd * g_d.unsqueeze(-1)).sum(dim=1)

        # [ì „ëµ ìˆ˜ì •] Warm-up Rotation Refinement
        # iter_idxê°€ 2(í˜¹ì€ 1) ì´ìƒì¼ ë•Œë§Œ íšŒì „ ì—…ë°ì´íŠ¸ë¥¼ ì°¨ë‹¨í•©ë‹ˆë‹¤.
        # ì´ˆë°˜ 0, 1íšŒì°¨ ë£¨í”„ì—ì„œëŠ” Rë„ í•¨ê»˜ ìµœì í™”í•˜ì—¬ Tì™€ì˜ ì •ë ¬ì„ ë§ì¶¥ë‹ˆë‹¤.
        if iter_idx >= 2:
            # íšŒì „(3,4,5)ì— ëŒ€í•œ Gradientë¥¼ 0ìœ¼ë¡œ ì„¤ì •
            g_eff[:, 3:] = 0.0
            # íšŒì „ ëŒ€ê° ì„±ë¶„ì— í° ê°’ì„ ë”í•´ ì—…ë°ì´íŠ¸ ì–µì œ
            H_eff[:, 3:, 3:] += 1e8 

        # 6. Ridge Damping ê°•í™”
        eps_ridge = 1.0
        H_eff = H_eff + eps_ridge * torch.eye(6, device=device).unsqueeze(0)
        H_eff = H_eff + torch.diag_embed(torch.diagonal(H_eff, dim1=-2, dim2=-1) * 0.01)

        # 7. Linear System Solve
        try:
            delta_pose = torch.linalg.solve(H_eff, g_eff) 
        except RuntimeError:
            delta_pose = torch.zeros_like(g_eff)

        # 8. ìµœì¢… delta_depth ê³„ì‚°
        v = torch.matmul(H_pd.transpose(-1, -2), delta_pose.unsqueeze(1)).squeeze(-1)
        delta_depth = inv_H_dd * (g_d - v)
        
        delta_pose = delta_pose.squeeze(-1)
        
        # [ë°©ì–´] ì—…ë°ì´íŠ¸ ê°’ ì œí•œ
        delta_pose = torch.clamp(delta_pose, min=-2.0, max=2.0)
        delta_depth = torch.clamp(delta_depth, min=-5.0, max=5.0)

        if torch.isnan(delta_pose).any(): delta_pose = torch.zeros_like(delta_pose)
        if torch.isnan(delta_depth).any(): delta_depth = torch.zeros_like(delta_depth)

        return delta_pose, delta_depth
    
class PoseDepthUpdater(nn.Module):
    def __init__(self, min_depth=0.1, max_depth=100.0):
        super().__init__()
        self.min_depth = min_depth
        self.max_depth = max_depth
    
    def forward(self, curr_pose, curr_depth, delta_pose, delta_depth, a_p, a_d, iter_idx):
        # 1. Depth Update
        safe_delta_d = torch.clamp(delta_depth, min=-5.0, max=5.0) 
        new_depth = torch.clamp(curr_depth + a_d * safe_delta_d, min=self.min_depth, max=self.max_depth)

        # 2. Pose Update (Conditional Rotation)
        scaled_delta = a_p * torch.tanh(delta_pose / 2.0) * 2.0
        
        # [ì „ëµ ìˆ˜ì •] ì´ˆë°˜ 2íšŒê¹Œì§€ë§Œ R ì—…ë°ì´íŠ¸ í—ˆìš©, ì´í›„ëŠ” Të§Œ ì—…ë°ì´íŠ¸
        if iter_idx < 2:
            # Rê³¼ T ëª¨ë‘ ì—…ë°ì´íŠ¸ (ê¸°í•˜í•™ì  ì •ë ¬)
            pure_delta = scaled_delta 
        else:
            # Të§Œ ì—…ë°ì´íŠ¸ (ìŠ¤ì¼€ì¼ ì •ë°€ ë³´ì •)
            pure_delta = torch.cat([
                scaled_delta[..., :3], 
                torch.zeros_like(scaled_delta[..., 3:])
            ], dim=-1)

        delta_SE3 = SE3.exp(pure_delta)
        new_pose = curr_pose * delta_SE3 
        return new_pose, new_depth  

class GraphUpdateBlock(nn.Module):
    def __init__(self, input_dim=256, hidden_dim=256):
        super().__init__()
        self.hidden_dim = hidden_dim
        # Node(256) + r(2) + tri_w(1) + vp_s(1) = 260
        self.spatial_gat = GeometricGAT(in_channels=260, hidden_dim=hidden_dim)
        self.norm_gat = nn.LayerNorm(hidden_dim)
        self.gru = nn.GRUCell(input_size=hidden_dim, hidden_size=hidden_dim)
        self.norm_h = nn.LayerNorm(hidden_dim)
        
        self.head = nn.Sequential(
            nn.Linear(hidden_dim, 128),
            nn.LayerNorm(128),
            nn.SiLU(),
            nn.Linear(128, 4) # conf, a_p, a_d, (spare)
        )

    def forward(self, h, node_feat, r, tri_w, vp_s, edges, edge_attr, intrinsics, kpts, pts_3d):
        B, N, _ = node_feat.shape
        
        # r_norm ë° ì…ë ¥ ê²°í•©
        r_norm = r / (intrinsics[:, :2].unsqueeze(1) + 1e-8) 
        x_fused = torch.cat([node_feat, r_norm, tri_w, vp_s], dim=-1)
        x_fused = torch.where(torch.isnan(x_fused), torch.zeros_like(x_fused), x_fused)

        # Spatial GAT
        x_spatial_flat, _, _ = self.spatial_gat(
            x=x_fused.reshape(-1, 260), 
            edge_index=edges, edge_attr=edge_attr,
            kpts=kpts.reshape(-1, 2), pts_3d=pts_3d.reshape(-1, 3)
        )
        
        x_spatial = self.norm_gat(x_spatial_flat.view(B, N, -1))
        h_new_flat = self.gru(
            torch.clamp(x_spatial, -10.0, 10.0).reshape(-1, self.hidden_dim), 
            torch.clamp(h, -10.0, 10.0).reshape(-1, self.hidden_dim)
        )
        # Temporal GRU
        h_new_flat = self.gru(x_spatial.reshape(-1, self.hidden_dim), h.reshape(-1, self.hidden_dim))
        h_new = self.norm_h(torch.clamp(h_new_flat, -50.0, 50.0).view(B, N, -1))

        # Decision
        out = self.head(h_new)
        conf = torch.sigmoid(out[..., 0:1])
        a_p = torch.sigmoid(out[..., 1:2]).mean(dim=1) * 0.1
        a_d = torch.sigmoid(out[..., 2:3]) * 0.1
        
        return h_new, conf, a_p, a_d



