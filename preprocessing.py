import os
import torch
import numpy as np
import cv2
from tqdm import tqdm
from torch.utils.data import Dataset, DataLoader
import multiprocessing as mp
from scipy.spatial import Delaunay

from src.model import VO
from CFG.vo_cfg import vo_cfg

import os
import cv2
import torch
import numpy as np
from torch.utils.data import Dataset

def read_calib_file(path):
    """KITTI calib.txt íŒŒì¼ì„ ì½ì–´ ë”•ì…”ë„ˆë¦¬ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤."""
    float_chars = set("0123456789.e+- ")
    data = {}
    with open(path, 'r') as f:
        for line in f.readlines():
            key, value = line.split(':', 1)
            value = value.strip()
            data[key] = np.array([float(x) for x in value.split()])
    return data

class PreprocessDataset(Dataset):
    def __init__(self, data_root, sequences):
        self.data_root = data_root
        self.samples = []
        self.calib_dict = {}  

        for seq in sequences:
            calib_path = os.path.join(data_root, seq, 'calib.txt')
            if not os.path.exists(calib_path):
                print(f"âš ï¸ {seq}: calib.txtë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                continue
            
            calib_data = read_calib_file(calib_path)
            P2 = np.reshape(calib_data['P2'], (3, 4))
            fx, fy, cx, cy = P2[0, 0], P2[1, 1], P2[0, 2], P2[1, 2]

            img_dir = os.path.join(data_root, seq, 'image_2')
            if not os.path.exists(img_dir): continue
            
            img_names = sorted([f for f in os.listdir(img_dir) if f.endswith('.png')])
            first_img = cv2.imread(os.path.join(img_dir, img_names[0]))
            H_raw = first_img.shape[0]

            cy_corrected = cy - (H_raw % 32)
            
            self.calib_dict[seq] = np.array([
                [fx, 0,  cx],
                [0,  fy, cy_corrected],
                [0,  0,  1]
            ], dtype=np.float32)

            # 4. ìƒ˜í”Œ ë¦¬ìŠ¤íŠ¸ ìƒì„±
            for name in img_names:
                self.samples.append({
                    'seq': seq,
                    'imgnum': int(name.split('.')[0]),
                    'img_path_2': os.path.join(data_root, seq, 'image_2', name),
                    'img_path_3': os.path.join(data_root, seq, 'image_3', name)
                })

    def __len__(self):
        return len(self.samples)

    def __getitem__(self, idx):
        s = self.samples[idx]
        seq = s['seq']
        imgs_processed = []
        
        for p in [s['img_path_2'], s['img_path_3']]:
            img = cv2.imread(p)
            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
            H, W, _ = img.shape
            
            top_crop = H % 32
            img = img[top_crop:, :1216]
            
            
            imgs_processed.append(torch.from_numpy(img).permute(2, 0, 1).float() / 255.0)
        
        return {
            'images': torch.stack(imgs_processed), 
            'seq': seq,
            'imgnum': s['imgnum'],
            'K': self.calib_dict[seq] 
        }

def save_worker(task_data):
    try:
        kpts = task_data['kpts'] # [N, 2]
        descs = task_data['node_features'] # [N, 256]
        scores = task_data['scores'] # [N]
        
        # 1. ìœ íš¨ì„± ë§ˆìŠ¤í¬ ìƒì„± (íŒ¨ë”©ëœ 0ì¢Œí‘œ ì œì™¸)
        mask = np.linalg.norm(kpts, axis=1) > 0
        
        # 2. Delaunay Triangulation ë° ì‚¼ê°í˜• ë²•ì„  ë²¡í„° (R ì¶”ì •ìš©)
        valid_kpts = kpts[mask]
        tri_normals = np.array([])
        tri_indices = np.array([])
        
        if len(valid_kpts) >= 3:
            dt = Delaunay(valid_kpts)
            tri_indices = dt.simplices
            # ë‹¨ìˆœ ì˜ˆì‹œ: ì‚¼ê°í˜•ì˜ ì™¸ì ì„ í†µí•œ ê°€ìƒ ë²•ì„  ë²¡í„° ê³„ì‚° (ì†Œì‹¤ì  íˆ¬í‘œ í™œìš©)
            # ì‹¤ì œ ì—°êµ¬ ìˆ˜ì‹ì— ë”°ë¼ ì´ ë¶€ë¶„ì„ tri_normals ê³„ì‚° ë¡œì§ìœ¼ë¡œ êµì²´í•˜ì„¸ìš”.
            tri_normals = np.zeros((len(tri_indices), 3)) 

        full_save_path = os.path.join(task_data['save_dir'], task_data['rel_path'] + ".npz")
        os.makedirs(os.path.dirname(full_save_path), exist_ok=True)

        np.savez_compressed(
            full_save_path,
            kpts=kpts.astype(np.float32),
            descs=descs.astype(np.float16),
            scores=scores.astype(np.float32),
            mask=mask.astype(np.bool_),
            tri_indices=tri_indices.astype(np.int32),
            tri_normals=tri_normals.astype(np.float32),
            K=task_data['K'].astype(np.float32),
            image_size=np.array([352, 1216], dtype=np.float32)
        )
    except Exception as e:
        print(f"âŒ ì €ì¥ ì—ëŸ¬: {e}")

# --- [3] ë³‘ë ¬ ì¶”ì¶œ ë£¨í”„ ---
@torch.no_grad()
def export_parallel(model, dataloader, save_dir, num_cpu):
    model.eval()
    device = torch.device('cuda')
    model.to(device)
    pool = mp.Pool(processes=num_cpu)
    async_results = []

    for batch in tqdm(dataloader, desc="âš™ï¸ í†µí•© ì „ì²˜ë¦¬ ì¤‘"):
        images = batch['images'].to(device)
        B = images.shape[0]
        K_batch = batch['K'].numpy()

        for side_idx, side_name in zip([0, 1], ['image_2', 'image_3']):
            # [ìˆ˜ì •] ëª¨ë¸ì—ì„œ ì ìˆ˜(scores)ê¹Œì§€ í•¨ê»˜ ì¶”ì¶œí•˜ë„ë¡ ì¸í„°í˜ì´ìŠ¤ í™•ì¸ í•„ìš”
            kpts, descs = model.extractor(images[:, side_idx])
            # SuperPointì—ì„œ scoreë¥¼ ë”°ë¡œ ë½‘ëŠ” ë¡œì§ì´ ì—†ë‹¤ë©´ ì„ì˜ì˜ 1.0ìœ¼ë¡œ ì´ˆê¸°í™” ê°€ëŠ¥
            scores = torch.ones(B, kpts.shape[1], device=device) 

            tasks = []
            for b in range(B):
                rel_path = os.path.join(batch['seq'][b], side_name, f"{int(batch['imgnum'][b]):06d}")
                tasks.append({
                    'kpts': kpts[b].cpu().numpy(),
                    'node_features': descs[b].cpu().numpy(),
                    'scores': scores[b].cpu().numpy(),
                    'K': K_batch[b],
                    'rel_path': rel_path,
                    'save_dir': save_dir
                })
            
            res = pool.map_async(save_worker, tasks)
            async_results.append(res)

        if len(async_results) > 40:
            for r in async_results[:20]: r.wait()
            async_results = async_results[20:]

    pool.close()
    pool.join()

if __name__ == "__main__":
    # 1. ê²½ë¡œ ì„¤ì • (ì‚¬ìš©ì í™˜ê²½ì— ë§ì¶° í™•ì¸ í•„ìš”)
    RAW_DATA_PATH = "/home/jnu-ie/Dataset/kitti_odometry/data_odometry_color/dataset/sequences" 
    SAVE_PATH = "/home/jnu-ie/kys/Geo-VO/gendata/precomputed"
    
    # ì²˜ë¦¬í•  ì‹œí€€ìŠ¤ ë¦¬ìŠ¤íŠ¸ (00~08)
    SEQUENCES = [f"{i:02d}" for i in range(9)] 
    
    # 2. ëª¨ë¸ ë° ì„¤ì • ì´ˆê¸°í™”
    # ì „ì²˜ë¦¬ ì‹œì—ëŠ” precomputed ë°ì´í„°ë¥¼ ì“°ì§€ ì•Šìœ¼ë¯€ë¡œ False ì„¤ì •
    vo_cfg.use_precomputed = False 
    
    print("Geo-VO ëª¨ë¸ ë¡œë“œ ì¤‘...")
    model = VO(vo_cfg)
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model.to(device)
    
    # 3. ë°ì´í„°ì…‹ ë° ë°ì´í„°ë¡œë” ìƒì„±
    dataset = PreprocessDataset(RAW_DATA_PATH, SEQUENCES)
    print(f"ğŸ” ì´ ìƒ˜í”Œ ìˆ˜: {len(dataset)}")
    
    if len(dataset) == 0:
        print("ë°ì´í„°ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. RAW_DATA_PATHë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
    else:
        # num_workersëŠ” ë°ì´í„° ë¡œë“œ ë³‘ë ¬í™”, num_cpuëŠ” ì €ì¥(save_worker) ë³‘ë ¬í™”ì— ì‚¬ìš©ë©ë‹ˆë‹¤.
        dataloader = DataLoader(
            dataset, 
            batch_size=vo_cfg.batchsize, 
            num_workers=4, 
            shuffle=False,
            drop_last=False
        )
        
        print(f"ì „ì²˜ë¦¬ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤. (ì €ì¥ ê²½ë¡œ: {SAVE_PATH})")
        export_parallel(
            model=model, 
            dataloader=dataloader, 
            save_dir=SAVE_PATH, 
            num_cpu=vo_cfg.num_cpu
        )
        
        print(f"\nëª¨ë“  ì‹œí€€ìŠ¤ ì „ì²˜ë¦¬ ì™„ë£Œ!")
        print(f"ê²°ê³¼ë¬¼ í™•ì¸: {os.listdir(SAVE_PATH)}")