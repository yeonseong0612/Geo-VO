import os
import torch
import torch.nn as nn
import torch.optim as optim
import torch.distributed as dist
import torch.multiprocessing as mp
from torch.nn.parallel import DistributedDataParallel as DDP
from torch.utils.data import DataLoader, DistributedSampler
from tqdm import tqdm 
from datetime import datetime

from CFG.vo_cfg import vo_cfg as cfg
from src.model import VO
from src.loader import DataFactory, vo_collate_fn
from src.loss import total_loss

def setup(rank, world_size):
    os.environ['MASTER_ADDR'] = 'localhost'
    os.environ['MASTER_PORT'] = '12355'
    dist.init_process_group("nccl", rank=rank, world_size=world_size)
    torch.cuda.set_device(rank)

def cleanup():
    dist.destroy_process_group()

def train(rank, world_size, cfg):
    is_ddp = world_size > 1
    if is_ddp:
        setup(rank, world_size)
    
    device = torch.device(f"cuda:{rank}")
    
    # 1. ë°ì´í„° ë¡œë” ì„¤ì •
    dataset = DataFactory(cfg, mode='train')
    sampler = DistributedSampler(dataset, num_replicas=world_size, rank=rank) if is_ddp else None
    loader = DataLoader(
        dataset, 
        batch_size=cfg.batchsize, 
        shuffle=(sampler is None),
        num_workers=4, # ì„±ëŠ¥ì„ ìœ„í•´ 4 ì •ë„ë¡œ ìƒí–¥ ê¶Œìž¥
        sampler=sampler, 
        collate_fn=vo_collate_fn,
        pin_memory=True
    )

    # 2. ëª¨ë¸ ì„¤ì •
    model = VO(cfg).to(device)
    
    # [ìˆ˜ì •] ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ (ì„±ê³µì ì´ì—ˆë˜ ì—í¬í¬ 4 ë¶ˆëŸ¬ì˜¤ê¸°)
    checkpoint_path = "./checkpoint/geovo_epoch_4.pth"
    start_epoch = 0
    
    if os.path.exists(checkpoint_path):
        checkpoint = torch.load(checkpoint_path, map_location=device)
        # DDP ì €ìž¥ ë°©ì‹ì— ë”°ë¼ 'module.' ì ‘ë‘ì‚¬ ì œê±°ê°€ í•„ìš”í•  ìˆ˜ ìžˆìŒ
        state_dict = checkpoint['model_state_dict']
        model.load_state_dict(state_dict)
        start_epoch = checkpoint['epoch'] + 1
        if rank == 0:
            print(f"âœ… ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ ì„±ê³µ: {checkpoint_path} (ì—í¬í¬ {start_epoch}ë¶€í„° ìž¬ê°œ)")
            
    # [í•µì‹¬] log_lmbda ê³ ì • (ìˆ˜ì¹˜ì  ì•ˆì •ì„± í™•ë³´)
    model.log_lmbda.requires_grad = False
    
    if is_ddp:
        model = DDP(model, device_ids=[rank], find_unused_parameters=True)
        raw_model = model.module
    else:
        raw_model = model
    
    # 3. ì˜µí‹°ë§ˆì´ì € ì„¤ì •
    optimizer = optim.AdamW(
        filter(lambda p: p.requires_grad, model.parameters()), 
        lr=cfg.learning_rate * 0.2, 
        weight_decay=cfg.weight_decay
    )
    scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=cfg.MultiStepLR_milstone, gamma=cfg.MultiStepLR_gamma)

    # 4. ë¡œê·¸ íŒŒì¼ ì„¤ì •
    log_file = None
    if rank == 0:
        if not os.path.exists(cfg.logdir): os.makedirs(cfg.logdir)
        log_path = os.path.join(cfg.logdir, f"train_log_{datetime.now().strftime('%m%d_%H%M')}.txt")
        log_file = open(log_path, "w")
        print(f"ðŸš€ Fine-tuning ì‹œìž‘ | GPU ê°œìˆ˜: {world_size} | ë¡œê·¸: {log_path}")

    # í•™ìŠµ ë£¨í”„
    for epoch in range(start_epoch, cfg.maxepoch):
        if is_ddp: sampler.set_epoch(epoch)
        model.train()
        
        # [ì—ëŸ¬ í•´ê²°] ê° ì—í­ ì‹œìž‘ ì‹œ ëª¨ë‹ˆí„°ë§ ë³€ìˆ˜ ì´ˆê¸°í™”
        avg_loss, avg_t, avg_r = 0.0, 0.0, 0.0
        epoch_loss, epoch_t, epoch_r = 0.0, 0.0, 0.0
        
        pbar = tqdm(loader, desc=f"Epoch {epoch}", disable=(rank != 0))
        
        for i, batch in enumerate(pbar):
            for k, v in batch.items():
                if isinstance(v, torch.Tensor):
                    batch[k] = v.to(device)
            
            optimizer.zero_grad()
            
            # ëª¨ë¸ ì¶”ë¡  (iters=4 ìœ ì§€, ìž¬íˆ¬ì˜ ì˜¤ì°¨ í¬í•¨)
            outputs = model(batch, iters=4, mode='train')
            
            # [ìˆ˜ì •] total_loss ë°˜í™˜ê°’ ê°œìˆ˜ ì¼ì¹˜ (final_loss, t_err, r_err, l_weight)
            loss, t_err, r_err, l_weight = total_loss(outputs, batch)

            if torch.isnan(loss):
                print(f"âš ï¸ Skip NaN Loss at Epoch {epoch}, Batch {i}")
                continue

            loss.backward()

            # [í•µì‹¬] Gradient Clipping: 0.94m ì •ì²´ê¸° ëŒíŒŒ ì‹œ ê°‘ìž‘ìŠ¤ëŸ¬ìš´ í­ì£¼ ë°©ì§€
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)

            optimizer.step()
            
            # í†µê³„ ì—…ë°ì´íŠ¸
            loss_val = loss.item()
            epoch_loss += loss_val
            epoch_t += t_err
            epoch_r += r_err
            
            if rank == 0:
                # ì´ë™ í‰ê·  ê³„ì‚°
                avg_loss = (avg_loss * i + loss_val) / (i + 1)
                avg_t = (avg_t * i + t_err) / (i + 1)
                avg_r = (avg_r * i + r_err) / (i + 1)

                pbar.set_postfix({
                    'L(avg/cur)': f"{avg_loss:.3f}/{loss_val:.3f}",
                    'T(avg/cur)': f"{avg_t:.3f}/{t_err:.3f}m",
                    'R(avg/cur)': f"{avg_r:.4f}/{r_err:.4f}r"
                })

        scheduler.step()

        # ì—í¬í¬ ì¢…ë£Œ í›„ ì €ìž¥ ë° ê¸°ë¡
        if rank == 0:
            final_avg_loss = epoch_loss / len(loader)
            final_avg_t = epoch_t / len(loader)
            final_avg_r = epoch_r / len(loader)
            
            log_str = f"[Epoch {epoch}] Avg Loss: {final_avg_loss:.4f}, Avg T: {final_avg_t:.4f}m, Avg R: {final_avg_r:.6f}rad\n"
            log_file.write(log_str)
            log_file.flush()

            checkpoint_dir = "./checkpoint/5"
            if not os.path.exists(checkpoint_dir): os.makedirs(checkpoint_dir)
            
            save_path = os.path.join(checkpoint_dir, f"geovo_epoch_{epoch}.pth")
            torch.save({
                'epoch': epoch,
                'model_state_dict': raw_model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'loss': final_avg_loss,
                'log_lmbda': raw_model.log_lmbda.data
            }, save_path)
            print(f"ðŸ’¾ Epoch {epoch} ëª¨ë¸ ì €ìž¥ ì™„ë£Œ: {save_path}")

    if is_ddp: cleanup()
    if log_file: log_file.close()

def main():
    world_size = torch.cuda.device_count()
    if world_size > 1:
        mp.spawn(train, args=(world_size, cfg), nprocs=world_size, join=True)
    else:
        train(0, 1, cfg)

if __name__ == "__main__":
    main()